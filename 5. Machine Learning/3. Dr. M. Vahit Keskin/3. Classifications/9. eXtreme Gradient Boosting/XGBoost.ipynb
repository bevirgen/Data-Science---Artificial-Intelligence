{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns\n",
    "from skompiler import skompile\n",
    "from sklearn.preprocessing import scale, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score,roc_curve\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv('diabetes.csv')\n",
    "df = diabetes.copy()\n",
    "y = df[\"Outcome\"]\n",
    "X = df.drop(['Outcome'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.30, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "              max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "              n_jobs=1, nthread=None, objective='binary:logistic',\n",
       "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "              seed=None, silent=True, subsample=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7489177489177489"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = xgb_model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "              max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "              n_jobs=1, nthread=None, objective='binary:logistic',\n",
       "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "              seed=None, silent=True, subsample=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        XGBClassifier\n",
       "\u001b[0;31mString form:\u001b[0m\n",
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "           colsample_bytr <...> reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "           seed=None, silent=True, subsample=1)\n",
       "\u001b[0;31mFile:\u001b[0m        /opt/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py\n",
       "\u001b[0;31mDocstring:\u001b[0m  \n",
       "Implementation of the scikit-learn API for XGBoost classification.\n",
       "\n",
       "    Parameters\n",
       "----------\n",
       "max_depth : int\n",
       "    Maximum tree depth for base learners.\n",
       "learning_rate : float\n",
       "    Boosting learning rate (xgb's \"eta\")\n",
       "n_estimators : int\n",
       "    Number of boosted trees to fit.\n",
       "silent : boolean\n",
       "    Whether to print messages while running boosting.\n",
       "objective : string or callable\n",
       "    Specify the learning task and the corresponding learning objective or\n",
       "    a custom objective function to be used (see note below).\n",
       "booster: string\n",
       "    Specify which booster to use: gbtree, gblinear or dart.\n",
       "nthread : int\n",
       "    Number of parallel threads used to run xgboost.  (Deprecated, please use n_jobs)\n",
       "n_jobs : int\n",
       "    Number of parallel threads used to run xgboost.  (replaces nthread)\n",
       "gamma : float\n",
       "    Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
       "min_child_weight : int\n",
       "    Minimum sum of instance weight(hessian) needed in a child.\n",
       "max_delta_step : int\n",
       "    Maximum delta step we allow each tree's weight estimation to be.\n",
       "subsample : float\n",
       "    Subsample ratio of the training instance.\n",
       "colsample_bytree : float\n",
       "    Subsample ratio of columns when constructing each tree.\n",
       "colsample_bylevel : float\n",
       "    Subsample ratio of columns for each split, in each level.\n",
       "reg_alpha : float (xgb's alpha)\n",
       "    L1 regularization term on weights\n",
       "reg_lambda : float (xgb's lambda)\n",
       "    L2 regularization term on weights\n",
       "scale_pos_weight : float\n",
       "    Balancing of positive and negative weights.\n",
       "base_score:\n",
       "    The initial prediction score of all instances, global bias.\n",
       "seed : int\n",
       "    Random number seed.  (Deprecated, please use random_state)\n",
       "random_state : int\n",
       "    Random number seed.  (replaces seed)\n",
       "missing : float, optional\n",
       "    Value in the data which needs to be present as a missing value. If\n",
       "    None, defaults to np.nan.\n",
       "**kwargs : dict, optional\n",
       "    Keyword arguments for XGBoost Booster object.  Full documentation of parameters can\n",
       "    be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.md.\n",
       "    Attempting to set a parameter via the constructor args and **kwargs dict simultaneously\n",
       "    will result in a TypeError.\n",
       "    Note:\n",
       "        **kwargs is unsupported by Sklearn.  We do not guarantee that parameters passed via\n",
       "        this argument will interact properly with Sklearn.\n",
       "\n",
       "Note\n",
       "----\n",
       "A custom objective function can be provided for the ``objective``\n",
       "parameter. In this case, it should have the signature\n",
       "``objective(y_true, y_pred) -> grad, hess``:\n",
       "\n",
       "y_true: array_like of shape [n_samples]\n",
       "    The target values\n",
       "y_pred: array_like of shape [n_samples]\n",
       "    The predicted values\n",
       "\n",
       "grad: array_like of shape [n_samples]\n",
       "    The value of the gradient for each sample point.\n",
       "hess: array_like of shape [n_samples]\n",
       "    The value of the second derivative for each sample point\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "        'n_estimators': [100, 500, 1000, 2000],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5,6],\n",
    "        'learning_rate': [0.1,0.01,0.02,0.05]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier()\n",
    "\n",
    "xgb_cv_model = GridSearchCV(xgb, xgb_params, cv = 10, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 192 candidates, totalling 1920 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 194 tasks      | elapsed:   25.5s\n",
      "[Parallel(n_jobs=-1)]: Done 397 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 680 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1045 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1490 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1920 out of 1920 | elapsed:  6.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score=nan,\n",
       "             estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                     colsample_bylevel=1, colsample_bytree=1,\n",
       "                                     gamma=0, learning_rate=0.1,\n",
       "                                     max_delta_step=0, max_depth=3,\n",
       "                                     min_child_weight=1, missing=None,\n",
       "                                     n_estimators=100, n_jobs=1, nthread=None,\n",
       "                                     objective='binary:logistic',\n",
       "                                     random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                                     scale_pos_weight=1, seed=None, silent=True,\n",
       "                                     subsample=1),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.1, 0.01, 0.02, 0.05],\n",
       "                         'max_depth': [3, 4, 5, 6],\n",
       "                         'n_estimators': [100, 500, 1000, 2000],\n",
       "                         'subsample': [0.6, 0.8, 1.0]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_cv_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 500, 'subsample': 1.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_cv_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(learning_rate = 0.01, \n",
    "                    max_depth = 6,\n",
    "                    min_samples_split = 2,\n",
    "                    n_estimators = 100,\n",
    "                    subsample = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_tuned =  xgb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7575757575757576"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = xgb_tuned.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
